The problem statement
=====================

Consider a plane that is divided into two half planes by a line,
where one half is red and the other half is green.

This is the ideal picture and we have to find that line.

However, our input data has errors in the x‚ÇÅ coordinates of each (x‚ÇÄ,x‚ÇÅ) point;
each point is vertically spread out according to a given probability distribution.

We will be using (assuming) the [logistic distribution](https://en.wikipedia.org/wiki/Logistic_distribution),
which is much like the normal distribution just a bit more forgiving for outliers.

The logistic distribution is given by

                 e(t;Œº,s)
  f(t;Œº,s) = ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº
              s (1 + e(t;Œº,s))¬≤

where
              (-(t-Œº)/s)
  e(t;Œº,s) = e

Let the equation of the line be given by

  x‚ÇÅ‚Çï = Œ≥ x‚ÇÄ‚Çï + Œ≤

where the h subscript stands for 'half'. After all, on this
line half of the points will be red and half will be green
(symmetry argument).


Probability density per color
=============================

We can now calculate the probability density of green points (f_green)
at the coordinates x‚ÇÄ,x‚ÇÅ. It will be a convolution product between the
chance that there was a green point before blurring at (x‚ÇÄ,y) times the
logistic distribution function around that point (f(y;0,s)).

  f_green(x‚ÇÄ,x‚ÇÅ) = (H ‚àó f)(x‚ÇÄ,x‚ÇÅ;s)

where H(y) is the (piecewise constant) heaviside step function,
0 for y < 0, 1/2 for y = 0 and 1 for y > 0. It represents the
chance that a points was green at y before blurring; in other
words y here represents the distance that the starting point is
above the half-line. Note that we should use Œº=0 for the
logistic distribution function f here.

Writing out this convolution product as an integral then gives
                    ‚àû
                   ‚å†
  f_green(x‚ÇÄ,x‚ÇÅ) = ‚éÆ H(y - (Œ≥ x‚ÇÄ + Œ≤)) f(x‚ÇÅ-y;0,s) dy =
                   ‚å°
                 -‚àû

                    ‚àû      (-(x‚ÇÅ-y)/s)
                   ‚å†      e                1
                 = ‚éÆ  ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº  ‚éº dy =
                   ‚å°  ‚éõ     (-(x‚ÇÅ-y)/s)‚éû2  s
            Œ≥ x‚ÇÄ + Œ≤  ‚éù1 + e           ‚é† 


Let u = (x‚ÇÅ-y)/s, which means dy = -s du. The limits of the
integral change accordingly: y = Œ≥ x‚ÇÄ + Œ≤ ‚Üí u = (x‚ÇÅ-(Œ≥ x‚ÇÄ + Œ≤))/s.
And y = ‚àû ‚Üí u = -‚àû.

                    -‚àû     (-u)
                   ‚å†    - e                 
                 = ‚éÆ  ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº du =
                   ‚å°  ‚éõ     (-u)‚éû2          
    (x‚ÇÅ-(Œ≥ x‚ÇÄ + Œ≤))/s ‚éù1 + e    ‚é† 

Reversing the limits of integration changes the sign:

   (x‚ÇÅ-(Œ≥ x‚ÇÄ + Œ≤))/s   (-u)
               ‚å†      e                 
             = ‚éÆ  ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº du
               ‚å°  ‚éõ     (-u)‚éû2          
             -‚àû   ‚éù1 + e    ‚é† 

This integral is the cumulative distribution function (CDF) of
the logistic distribution: the logistic sigmoid function.
Hence,
                       ‚é•(x‚ÇÅ-(Œ≥ x‚ÇÄ + Œ≤))/s 
  f_green(x‚ÇÄ,x‚ÇÅ) = œÉ(u)‚é•                 = œÉ((x‚ÇÅ-(Œ≥ x‚ÇÄ + Œ≤))/s)
                       ‚é•-‚àû 

because œÉ(-‚àû) = 0.

This means: very far from the line the chance to find only red
or green (depending on which side of the line) is 100%, and
precisely on the line, the chance to find red or green is 50%.

The probability density to find a red point is obviously

  f_red(x‚ÇÄ,x‚ÇÅ) = 1 - f_green(x‚ÇÄ,x‚ÇÅ)


Visualization of contour lines with equal probability density
=============================================================

The scale parameter s of the logistic distribution is proportional to its
standard deviation (œÉ = sœÄ/‚àö3). Where the standard deviation is defined as
the square root of the variance, the expectation value of the square of the
distances to the mean.

Note that this should not be interpreted as that the average vertical distance
that a point lays from the line is œÉ œÄ/‚àö3 s = 1.8138 s; the average distance
is 1.3863 s. Not that we're dealing with the logistic distribution directly
anyway; we can have green points at arbitrary large distance above the line
(and red ones at arbitrary distance below the line) because the probability
density to encounter red or green is the sigmoid function, as shown above.

Imagine a cloud of points that are either red or green, distributed as above.
Then we can draw a line through the plane where the probability that a point
is red or green is equal (0.5): the half-line x‚ÇÅ‚Çï = Œ≥ x‚ÇÄ‚Çï + Œ≤.

In fact, any parallel line has a constant probability density for each color,
as given above (f_green and f_red). For example, we could draw two lines at a
distance equal to the standard deviation:

   x‚ÇÅ ^                    ______ line where 0.85982 is green and 0.14018 is red.
      |                  ‚üã  ‚Üë ___ œÉ = sœÄ/‚àö3
      | mostly green   ‚üã    |/
      |              ‚üã     _‚Üì____ line where 0.5 is green and 0.5 is red
      |            ‚üã     ‚üã
      |          ‚üã     ‚üã
      |        ‚üã     ‚üã     ______ line where 0.85982 is red and 0.14018 is green.
      |      ‚üã     ‚üã     ‚üã
      |    ‚üã     ‚üã     ‚üã
      |  ‚üã     ‚üã     ‚üã \
      |‚üã     ‚üã     ‚üã    \___ slope Œ≥
      |    ‚üã     ‚üã
      |  ‚üã     ‚üã
  h __|‚üã     ‚üã
      |    ‚üã
      |  ‚üã
      |‚üã         mostly red
      |
      |
      +-----------------------> x‚ÇÄ

Note that 0.85982 is the value of the probability density function f_green (f_red) at
the standard deviation (sœÄ/‚àö3) above (below) the half-line:

  f_green(x‚ÇÄ, Œ≥ x‚ÇÄ + Œ≤ + sœÄ/‚àö3) = œÉ(((Œ≥ x‚ÇÄ + Œ≤ + sœÄ/‚àö3)-(Œ≥ x‚ÇÄ + Œ≤))/s) = œÉ(œÄ/‚àö3) = 0.85982

While the half line is defined by

  x‚ÇÅ‚Çï = Œ≥ x‚ÇÄ‚Çï + Œ≤

The other two lines are x‚ÇÅ‚Çä = Œ≥ x‚ÇÄ‚Çä + Œ≤ + s œÄ/‚àö3, and x‚ÇÅ‚Çã = Œ≥ x‚ÇÄ‚Çã + Œ≤ - s œÄ/‚àö3.


Relationship between trainable parameters (weights and biases) and Œ≥, Œ≤ and s
=============================================================================

                   ‚é°x‚ÇÄ‚é§
Let the vector X = ‚é¢x‚ÇÅ‚é• 
                   ‚é£1 ‚é¶

And matrix W = [ w‚ÇÄ w‚ÇÅ b ]

We want to following:
                         ‚éß œÄ/‚àö3  if (x‚ÇÄ, x‚ÇÅ) on the top line.
  WX = w‚ÇÄx‚ÇÄ + w‚ÇÅx‚ÇÅ + b = ‚é® 0     if (x‚ÇÄ, x‚ÇÅ) on the half line.
                         ‚é© -œÄ/‚àö3 if (x‚ÇÄ, x‚ÇÅ) on the bottom line.

So that the raw neuron output can directly be plugged into a sigmoid
function to get its current representation of f_green (f_red).

For points on the top and bottom line we have

  w‚ÇÄx‚ÇÄ + w‚ÇÅ(Œ≥ x‚ÇÄ + Œ≤ +  s œÄ/‚àö3) + b = + œÄ/‚àö3
  w‚ÇÄx‚ÇÄ + w‚ÇÅ(Œ≥ x‚ÇÄ + Œ≤ -  s œÄ/‚àö3) + b = - œÄ/‚àö3
  ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº subtract
     0 + w‚ÇÅ(  0  + 0 + 2s œÄ/‚àö3) + 0 = 2 œÄ/‚àö3     -->
   
    ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
    ‚îÇ w‚ÇÅ = 1/s  ‚áê‚áí  s = 1/w‚ÇÅ ‚îÇ
    ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
    
On the half line at x‚ÇÄ=0 we have:

  x‚ÇÅ = Œ≤

and

  w‚ÇÄ 0 + (1/s)(Œ≥ 0 + Œ≤) + b = 0 -->
  Œ≤/s + b = 0 -->

    ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
    ‚îÇ b = -Œ≤/s  ‚áê‚áí  Œ≤ = -b/w‚ÇÅ ‚îÇ
    ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

Finally, also using the half line,

  w‚ÇÄx‚ÇÄ + w‚ÇÅ(Œ≥ x‚ÇÄ + Œ≤) + b = 0

Devide by x‚ÇÄ, assuming it is non-zero:

  0 = w‚ÇÄ + w‚ÇÅ Œ≥ + w‚ÇÅ Œ≤ / x‚ÇÄ + b / x‚ÇÄ =

using Œ≤ = -b/w‚ÇÅ

  w‚ÇÄ + w‚ÇÅ Œ≥ + w‚ÇÅ (-b/w‚ÇÅ) / x‚ÇÄ + b / x‚ÇÄ =

  w‚ÇÄ + w‚ÇÅ Œ≥ =

devide by w‚ÇÅ

  s w‚ÇÄ + Œ≥ = 0

And thus

    ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
    ‚îÇ w‚ÇÄ = -Œ≥/s  ‚áê‚áí  Œ≥ = -w‚ÇÄ/w‚ÇÅ ‚îÇ
    ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

Now note that

  f_green(x‚ÇÄ,x‚ÇÅ) = œÉ((x‚ÇÅ-(Œ≥ x‚ÇÄ + Œ≤))/s) =
                   œÉ((x‚ÇÅ-((-w‚ÇÄ/w‚ÇÅ) x‚ÇÄ + (-b/w‚ÇÅ)))/(1/w‚ÇÅ)) =
                   œÉ(x‚ÇÅ w‚ÇÅ + w‚ÇÄ x‚ÇÄ + b)

In other words, the prediction is:

    ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
    ‚îÇ f_green(x‚ÇÄ,x‚ÇÅ) = œÉ(WX)   ‚îÇ
    ‚îÇ f_red(x‚ÇÄ,x‚ÇÅ) = 1 - œÉ(WX) ‚îÇ
    ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ


Understanding the signs
=======================

Let the line equation be W¬∑X = 0, where X = (x‚ÇÄ, x‚ÇÅ, 1) and W = (w‚ÇÄ, w‚ÇÅ, w‚ÇÇ).
Note w‚ÇÇ here plays the role of the bias, called b above. In this case X stands
for "points on the line".

Hence, w‚ÇÄ x‚ÇÄ + w‚ÇÅ x‚ÇÅ + w‚ÇÇ = 0. The vector N' = (w‚ÇÄ, w‚ÇÅ) is normal to the line.
Let N = (w‚ÇÄ, w‚ÇÅ, 0), so that it can be added and subtracted from X or P vectors
without influencing the added '1', which is necessary to deal with the bias
correctly.

If adding kN to a point P makes it end up on the line, then k is called the
'signed distance' in units of N. Let d = k|N|, then d is just the signed
distance (and |d| the actual (positive) distance).

W¬∑X is what is fed into the sigmoid activation function (in this case X stands
for the input of the neuron, aka - an arbitrary point; and W¬∑X will only be
zero for points X on the line). The output of that activation function is the
density of green points (or the probability that a given point is green) (by
definition). Thus, the larger the value of W¬∑X to more we strayed from the line
into the green area.

Adding N to a point X on the line causes W¬∑X to become larger: let X be a point
on the line (so W¬∑X = 0), then add N to it to get P: P = X + N. Then the
value of W¬∑P = W¬∑(X + N) = W¬∑X + W¬∑N = W¬∑N = w‚ÇÄ w‚ÇÄ + w‚ÇÅ w‚ÇÅ + w‚ÇÇ 0 = |N|.
Conclusion: the normal N points in the direction of the green area.

Decreasing the value of w‚ÇÇ therefore moves the line into the green area:
Let P = X + N, where X is on the old line (as above) then a line through
P would have the equation: W¬∑P = w‚ÇÄ p‚ÇÄ + w‚ÇÅ p‚ÇÅ + w‚ÇÇ' = 0, where p‚ÇÄ = x‚ÇÄ+w‚ÇÄ,
p‚ÇÅ = x‚ÇÅ+w‚ÇÅ (because P = X + N) and w‚ÇÇ' = w‚ÇÇ + Œîw‚ÇÇ. Hence

  0 = w‚ÇÄ(x‚ÇÄ+w‚ÇÄ) + w‚ÇÅ(x‚ÇÅ+w‚ÇÅ) + (w‚ÇÇ+Œîw‚ÇÇ) =
    = w‚ÇÄx‚ÇÄ + w‚ÇÅx‚ÇÅ + w‚ÇÇ + w‚ÇÄw‚ÇÄ + w‚ÇÅw‚ÇÅ + Œîw‚ÇÇ =
    = 0                + |N|         + Œîw‚ÇÇ

Thus Œîw‚ÇÇ = -|N| : decreasing the value of w‚ÇÇ causes the line to move into
the direction of N, into the green area.

A simplification
================

Before talking about gradients, lets simplify the problem
by recognizing that it doesn't really matter that Œ≥ and x‚ÇÄ
exist for the problem.

We can set Œ≥=0 for now (have just horizontal lines, and of
course have a red/green distribution as training data that
is not a function of x‚ÇÄ).

   x‚ÇÅ ^                    
      |
      |      mostly green
      |
      |________________________  ______ line where 0.85982 is green and 0.14018 is red.
      |                           ‚Üë ___ s œÄ/‚àö3                                         
      |                           |/                                                   
  0 __|________________________  _‚Üì____ line where 0.5 is green and 0.5 is red         
      |                                                                                
      |
      |________________________  ______ line where 0.85982 is red and 0.14018 is green.
      |
      |
      |      mostly red                                                                
      |
      +-----------------------> x‚ÇÄ

Since Œ≥=0, and we don't want to take it into account while learning,
we need to force w‚ÇÄ to 0. Note that under those circumstances Œº=Œ≤.

The network therefore has to fit

  WX = w‚ÇÅx‚ÇÅ + b, or f_green(x‚ÇÅ) = œÉ(w‚ÇÅx‚ÇÅ + b)

against the actual distribution that we assume to have Œ≤ = -b‚Çê/w‚Çê and s = 1/w‚Çê,
where w‚Çê and b‚Çê are the targets for w‚ÇÅ and b to be fit to ('a' stands for
"actual" distribution).

  f_green_actual(x‚ÇÅ) = œÉ((x‚ÇÅ-(Œ≥ x‚ÇÄ + Œ≤))/s) = œÉ(x‚ÇÅ w‚Çê + b‚Çê).

In otherwords, the best values for the trainable variables are w‚ÇÅ=w‚Çê and b=b‚Çê.

Since x‚ÇÄ has no influence anymore, we might as well
replace it with f_green in the plot:

   f_green
        ^                                             (sigmoid function)
     1  |                                , . , . . . .
        |                              ‚üã . '     
        |    Œº=-b‚Çê/w‚Çê,s=1/w‚Çê__        .        
        |                     \     .           
   0.5--| RED                  \  ‚üã /            GREEN
        |                       ‚üã  /\         
        |                     ‚üã   /  \__Œº=-b/w‚ÇÅ,s=1/w‚ÇÅ
        |                   .    .           
        |                 .                     
        |              .       .            
     0 _|.    .    .    . . .
        +------------------------+-+------------------>
                                 / \                   x‚ÇÅ
                           -b‚Çê/w‚Çê   -b/w‚ÇÅ

This shows two sigmoid functions œÉ((x‚ÇÅ-Œº)/s):
one with Œº=-b‚Çê/w‚Çê, s=1/w‚Çê which shows the actual ratio of green vs red
points (in the training set) and one with Œº=-b/w‚ÇÅ, s=1/w‚ÇÅ which is the
current sigmoid that the neuron is considering (the current prediction).

A loss function
===============

My idea now is to determine a "likeliness" that the training data would
happen given the predicted distribution.

Consider a single point at x‚ÇÅ=x. The chance that that point is green
given the current prediction is œÉ(x w‚ÇÅ + b), hence if it is green we
multiply the total likeliness with that value, and if it is red we
multiply it with (1 - œÉ(x w‚ÇÅ + b)).

Note that if we have a lot of points then the total "likeliness"
will go to zero, as we're only multiplying it with values less
than one for each point. However, this is not important, what
is important is that the MOST likely scenario, however small,
will happen for b=b‚Çê and w‚ÇÅ=w‚Çê.

Consider a very large number of points that are evenly distributed
over x‚ÇÅ, in fact, so many that we have M points for each interval Œîx.
Let x·µ¢ = i Œîx, but realize that we have M points with this x value.

The likeliness contribution for a given value of i then will be

  likeliness·µ¢ = P_green·µ¢^N_green·µ¢ * P_red·µ¢^N_red·µ¢

where P_green·µ¢ is the predicted probability that a point at x·µ¢
is green, and N_green·µ¢ is the number of points at x·µ¢ that are
actually green (the target color). Visa versa for red.

  P_green·µ¢ =   œÉ(x·µ¢ w‚ÇÅ + b)
  N_green·µ¢ = M œÉ(x·µ¢ w‚Çê + b‚Çê)
  P_red·µ¢   = 1 - P_green·µ¢
  N_red·µ¢   = M - N_green·µ¢

The total likeliness is then

  likeliness = Œ† likeliness·µ¢
               i  

Note again that for large M and/or small Œîx this goes to zero. But for
finite M the maximum (that hopefully will be at w‚ÇÅ=w‚Çê and b=b‚Çê) should,
though very small, be non-zero. Nevertheless, to deal with the small
value of the likeliness we will work with the natural log of it from
now on:
                     
  Log(likeliness) = ùö∫ Log(likeliness·µ¢) =
                    i

                  = ùö∫ ‚ßºN_green·µ¢ Log(P_green·µ¢) + N_red·µ¢ Log(P_red·µ¢)‚ßΩ =
                    i

                  = ùö∫ ‚ßºM œÉ(x·µ¢ w‚Çê + b‚Çê) Log(œÉ(x·µ¢ w‚ÇÅ + b)) + (M - M œÉ(x·µ¢ w‚Çê + b‚Çê)) Log(1 - œÉ(x·µ¢ w‚ÇÅ + b))‚ßΩ =
                    i

                  = M ùö∫ ‚ßºœÉ(x·µ¢ w‚Çê + b‚Çê) Log(œÉ(x·µ¢ w‚ÇÅ + b)) + (1 -   œÉ(x·µ¢ w‚Çê + b‚Çê)) Log(1 - œÉ(x·µ¢ w‚ÇÅ + b))‚ßΩ
                      i

The likeliness when changing w‚ÇÅ and/or b away from the best fit should
give a smaller value, but will be non-zero too unless w‚ÇÅ and/or b
are brought to a limit (that is, the Log(likeliness) will be very negative
but still fit in an int).

Therefore we first determine the optimal likeliness for w‚ÇÅ=w‚Çê and b=b‚Çê as
function of how many i values we use.

  Log(likeliness_opt) = M ùö∫ ‚ßºœÉ(x·µ¢ w‚Çê + b‚Çê) Log(œÉ(x·µ¢ w‚Çê + b‚Çê)) + (1 - œÉ(x·µ¢ w‚Çê + b‚Çê)) Log(1 - œÉ(x·µ¢ w‚Çê + b‚Çê))‚ßΩ
                          i

where we're interested to know what the sum does when x·µ¢ goes way beyond
the standard deviation, aka all the way to infinity will do.

In that case we can let Œîx approach zero and replace the sum with an integral:


  Log(likeliness) = M/Œîx ùö∫ ‚ßºœÉ(x·µ¢ w‚Çê + b‚Çê) Log(œÉ(x·µ¢ w‚ÇÅ + b)) Œîx‚ßΩ + M/Œîx ùö∫ ‚ßº(1 - œÉ(x·µ¢ w‚Çê + b‚Çê)) Log(1 - œÉ(x·µ¢ w‚ÇÅ + b)) Œîx‚ßΩ =
                         i                                   i
                          ‚àû                                           ‚àû                        
                         ‚å†                                           ‚å†                         
                  = M/Œîx ‚éÆ œÉ(x w‚Çê + b‚Çê) Log(œÉ(x w‚ÇÅ + b)) dx  +  M/Œîx ‚éÆ (1 - œÉ(x w‚Çê + b‚Çê)) Log(1 - œÉ(x w‚ÇÅ + b)) dx =
                         ‚å°                                           ‚å°                         
                       -‚àû                                          -‚àû

The optimal likeliness is
                              ‚àû                                            ‚àû
                             ‚å†                                            ‚å†                                           
  Log(likeliness_opt) = M/Œîx ‚éÆ œÉ(x w‚Çê + b‚Çê) Log(œÉ(x w‚Çê + b‚Çê)) dx  +  M/Œîx ‚éÆ (1 - œÉ(x w‚Çê + b‚Çê)) Log(1 - œÉ(x w‚Çê + b‚Çê)) dx =
                             ‚å°                                            ‚å°                                           
                           -‚àû                                           -‚àû                                            
Let u = x w‚Çê + b‚Çê, then du = w‚Çê dx. The integral limits don't change provided that w‚Çê is positive,
which it is because it is the reciprocal of s which is always positive.
                                   ‚àû                               ‚àû
                                  ‚å†                               ‚å† 
                      = M/(Œîx w‚Çê) ‚éÆ œÉ(u) Log(œÉ(u)) du + M/(Œîx w‚Çê) ‚éÆ (1 - œÉ(u)) Log(1 - œÉ(u)) du
                                  ‚å°                               ‚å° 
                                -‚àû                              -‚àû  

Let v = -u, dv = -du (and limits change sign) for the second integral, which then becomes,
using 1 - œÉ(-v) = 1 - (1 - œÉ(v)) = œÉ(v) :
    -‚àû                                 ‚àû                 
   ‚å†                                  ‚å†                  
 - ‚éÆ (1 - œÉ(-v)) Log(1 - œÉ(-v)) dv =  ‚éÆ œÉ(v) Log(œÉ(v)) dv
   ‚å°                                  ‚å°                  
  ‚àû                                 -‚àû                   
                                    ‚àû                 
                                   ‚å†                        3 œÄ¬≤ M
  Log(likeliness_opt) = 2M/(Œîx w‚Çê) ‚éÆ œÉ(u) Log(œÉ(u)) du = - ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº
                                   ‚å°                        Œîx w‚Çê
                                 -‚àû                   

Note that this result has been verified with the code in likeliness.cxx.

M, Œîx and w‚Çê (= 1/s‚Çê) are all "constants"; exclusively determined by the input (training) data.
The Log(likeliness) value that we'd get when w‚ÇÅ and b are not already optimally determined
should give us:

  Log(likeliness) = 2M/(Œîx w‚Çê) g(w‚ÇÅ, b)

where g depends on w‚Çê and b‚Çê as well.

Lets do the same thing again, but now for the full Log(likeliness) (see above):
                          ‚àû                                           ‚àû                        
                         ‚å†                                           ‚å†                         
  Log(likeliness) = M/Œîx ‚éÆ œÉ(x w‚Çê + b‚Çê) Log(œÉ(x w‚ÇÅ + b)) dx  +  M/Œîx ‚éÆ (1 - œÉ(x w‚Çê + b‚Çê)) Log(1 - œÉ(x w‚ÇÅ + b)) dx =
                         ‚å°                                           ‚å°                         
                       -‚àû                                          -‚àû
Again substitute u = x w‚Çê + b‚Çê, du = w‚Çê dx --> x = (u - b‚Çê)/w‚Çê, dx = 1/w‚Çê du
as well as v = -u, dv = -du.
                               ‚àû                                                    ‚àû                        
                              ‚å†                                                    ‚å†                         
                  = M/(Œîx w‚Çê) ‚éÆ œÉ(u) Log(œÉ(((u - b‚Çê)/w‚Çê) w‚ÇÅ + b)) du  +  M/(Œîx w‚Çê) ‚éÆ œÉ(v) Log(œÉ(-(((-v - b‚Çê)/w‚Çê) w‚ÇÅ + b))) dv =
                              ‚å°                                                    ‚å°                         
                            -‚àû                                                   -‚àû
                               ‚àû                                                        ‚àû                        
                              ‚å†                                                        ‚å†                         
                  = M/(Œîx w‚Çê) ‚éÆ œÉ(u) Log(œÉ(u w‚ÇÅ/w‚Çê + (b - b‚Çê w‚ÇÅ/w‚Çê))) du  +  M/(Œîx w‚Çê) ‚éÆ œÉ(v) Log(œÉ(v w‚ÇÅ/w‚Çê - (b - b‚Çê w‚ÇÅ/w‚Çê))) dv =
                              ‚å°                                                        ‚å°                         
                            -‚àû                                                       -‚àû

                  = 2M/(Œîx w‚Çê) g(w‚ÇÅ/w‚Çê, b - b‚Çê w‚ÇÅ/w‚Çê)

where we now noted that this function g can be a function of only the two parameters given.

In the special case that b and b‚Çê are zero the two integrals become the same, so that we can write

                  ‚àû                                        
                 ‚å†                                         
  g(w‚ÇÅ/w‚Çê, 0) =  ‚éÆ œÉ(u) Log(œÉ(u w‚ÇÅ/w‚Çê)) du 
                 ‚å°                                         
               -‚àû                                          

This integral is non-trivial (Mathematica couldn't do it) but I figured it out
(see `integral_sigma_log_sigma(long double we)` in likeliness.cxx):

                   œÄ¬≤ ((w‚ÇÅ/w‚Çê)¬≤ + 1)    -œÄ¬≤ ‚éõ w‚ÇÅ   w‚Çê ‚éû
  g(w‚ÇÅ/w‚Çê, 0) = - ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº = ‚éº‚éº‚éº‚éº ‚é¢ ‚éº‚éº + ‚éº‚éº ‚é•
                      12 (w‚ÇÅ/w‚Çê)        12  ‚éù w‚Çê   w‚ÇÅ ‚é† 


Note how this is symmetrical under the permutation w‚ÇÅ <--> w‚Çê.

In the special case that w‚ÇÅ=w‚Çê we can write:
                      ‚àû                                      ‚àû                                        
                     ‚å†                                      ‚å†                                         
  g(1, b - b‚Çê) = 1/2 ‚éÆ œÉ(u) Log(œÉ(u + (b - b‚Çê))) du  +  1/2 ‚éÆ œÉ(v) Log(œÉ(v - (b - b‚Çê))) dv =
                     ‚å°                                      ‚å°                                         
                   -‚àû                                     -‚àû                                          

These integrals are much harder even, but after adding both the result is so simple
that I also could guess it after numerically calculating it (again, see likeliness.cxx):

                 -œÄ¬≤   (b - b‚Çê)¬≤
  g(1, b - b‚Çê) = ‚éº‚éº‚éº - ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº
                  6       4

After that it wasn't that hard to guess the complete function too:

  long double g_weight_ratio_0 = integral_sigma_log_sigma(weight_ratio);
  return g_weight_ratio_0 - 0.25L / weight_ratio * bias_error * bias_error;

                            -œÄ¬≤ ‚éõ w‚ÇÅ   w‚Çê ‚éû   (b - b‚Çê w‚ÇÅ/w‚Çê)¬≤
  g(w‚ÇÅ/w‚Çê, b - b‚Çê w‚ÇÅ/w‚Çê) = ‚éº‚éº‚éº‚éº ‚é¢ ‚éº‚éº + ‚éº‚éº ‚é• - ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº
                            12  ‚éù w‚Çê   w‚ÇÅ ‚é†      4 (w‚ÇÅ/w‚Çê)

The largest likeliness (that is always less than zero) will give rise to the
largest value of Log(likeliness) (that is always negative nevertheless).

A good loss function is the negative of this Log(likeliness) expression,
where we can ignore any variable that only depends on the (constant) training
data. Aka, we can ignore 2M/(Œîx w‚Çê) and simply set the loss function equal
to -g(w‚ÇÅ/w‚Çê, b - b‚Çê w‚ÇÅ/w‚Çê) (also ignoring a factor of 4:

                 œÄ¬≤ ‚éõ w‚ÇÅ   w‚Çê ‚éû   (b - b‚Çê w‚ÇÅ/w‚Çê)¬≤   3 (w‚Çê b - b‚Çê w‚ÇÅ)¬≤ + œÄ¬≤ (w‚Çê¬≤ + w‚ÇÅ¬≤)
  loss(w‚ÇÅ, b) = ‚éº‚éº‚éº ‚é¢ ‚éº‚éº + ‚éº‚éº ‚é• + ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº = ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº
                 3  ‚éù w‚Çê   w‚ÇÅ ‚é†       (w‚ÇÅ/w‚Çê)                   3 w‚Çê w‚ÇÅ

The partial derivatives then are

            œÄ¬≤ ‚éõ 1    w‚Çê  ‚éû   b‚Çê¬≤   b¬≤ w‚Çê   œÄ¬≤/3 + b‚Çê¬≤   œÄ¬≤/3 + b¬≤ w‚Çê
  ‚àÇL/‚àÇw‚ÇÅ = ‚éº‚éº‚éº ‚é¢ ‚éº‚éº - ‚éº‚éº‚éº ‚é• + ‚éº‚éº‚éº - ‚éº‚éº‚éº‚éº‚éº = ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº - ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº ‚éº‚éº
            3  ‚éù w‚Çê   w‚ÇÅ¬≤ ‚é†   w‚Çê    w‚ÇÅ¬≤         w‚Çê          w‚ÇÅ     w‚ÇÅ

             ‚éõ       w‚Çê ‚éû
  ‚àÇL/‚àÇb = -2 ‚é¢b‚Çê - b ‚éº‚éº ‚é•
             ‚éù       w‚ÇÅ ‚é†

Setting these to zero shows that (we'd find), from the last one:

         w‚ÇÅ
  b = b‚Çê ‚éº‚éº
         w‚Çê

and then, filling that into the first one:

  œÄ¬≤/3 + b‚Çê¬≤   œÄ¬≤/3 + (b‚Çê(w‚ÇÅ/w‚Çê))¬≤ w‚Çê
  ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº = ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº ‚éº‚éº
      w‚Çê              w‚ÇÅ           w‚ÇÅ

Multiply both sides with w‚ÇÅ¬≤w‚Çê,

  (œÄ¬≤/3 + b‚Çê¬≤)w‚ÇÅ¬≤ = œÄ¬≤/3 w‚Çê¬≤ + b‚Çê¬≤w‚ÇÅ¬≤ --> œÄ¬≤/3 w‚ÇÅ¬≤ = œÄ¬≤/3 w‚Çê¬≤ --> w‚ÇÅ = +/- w‚Çê

In order for this to be a minimum, the second derivative must be positive.
The second derivative is:


  ‚àÇL¬≤/(‚àÇw‚ÇÅ)¬≤ = 2 (œÄ¬≤/3 + b¬≤) w‚Çê / w‚ÇÅ¬≥

which is only positive for the w‚ÇÅ = +w‚Çê solution.

Hence, using a gradient decent we will always end up with w‚ÇÅ=w‚Çê and b=b‚Çê,
the actual weight and bias.

This proves that -Log(likeliness) is a suitable and stable loss function.


Gradients
=========

Remember that the total likeliness was defined as

  likeliness = Œ† likeliness·µ¢
               i  
where

  likeliness·µ¢ = P_green·µ¢^N_green·µ¢ * P_red·µ¢^N_red·µ¢

Where i runs over the x-coordinates of a virtual "grid" of points.

But we can replace that with running over all the training data points.
Let x‚ÇÅ‚Çõ be the x‚ÇÅ coordinate of the s-th training Sample, with
color t‚Çõ (0 for red, 1 for green). Then the likeliness becomes

  likeliness = Œ† ‚ßºt‚Çõ P_green‚Çõ + (1 - t‚Çõ) P_red‚Çõ‚ßΩ
               s

That is, we multiply with either P_green‚Çõ or P_red‚Çõ, depending on
whether the point is green or red. Filling in P_green‚Çõ = œÉ(x‚ÇÅ‚Çõ w‚ÇÅ + b)
and P_red‚Çõ = 1 - P_green‚Çõ:

  likeliness = Œ† ‚ßºt‚Çõ œÉ(x‚ÇÅ‚Çõ w‚ÇÅ + b) + (1 - t‚Çõ)(1 - œÉ(x‚ÇÅ‚Çõ w‚ÇÅ + b))‚ßΩ
               s

The loss function then becomes:

  L = -Log(likeliness) = - ùö∫ Log(t‚Çõ z‚Çõ + (1 - t‚Çõ)(1 - z‚Çõ)) = - ùö∫ Log( (2t‚Çõ - 1)z‚Çõ + (1 - t‚Çõ) )
                              s                                   s

where z‚Çõ = œÉ(x‚ÇÅ‚Çõ w‚ÇÅ + b) is the result of the activation function.
Thus
                   1 - 2t‚Çõ                     1               -1
  ‚àÇL/‚àÇz‚Çõ = ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº, which is ‚éº‚éº‚éº‚éº‚éº‚éº if t=0 and ‚éº‚éº if t=1.
            1 - t‚Çõ - (1 - 2t‚Çõ) z‚Çõ            1 - z‚Çõ            z‚Çõ


  ‚àÇz‚Çõ/‚àÇw‚ÇÅ = z‚Çõ (1 - z‚Çõ) x‚ÇÅ‚Çõ

  ‚àÇz‚Çõ/‚àÇb =  z‚Çõ (1 - z‚Çõ)


  ‚àÇL/‚àÇw‚ÇÅ = ùö∫ ‚àÇL/‚àÇz‚Çõ ‚àÇz‚Çõ/‚àÇw‚ÇÅ = ùö∫ ((1 - t‚Çõ) z‚Çõ - t‚Çõ (1 - z‚Çõ)) x‚ÇÅ‚Çõ
           s                  s

  ‚àÇL/‚àÇb  = ùö∫ ‚àÇL/‚àÇz‚Çõ ‚àÇz‚Çõ/‚àÇb  = ùö∫ ((1 - t‚Çõ) z‚Çõ - t‚Çõ (1 - z‚Çõ))
           s                  s

Bringing back w‚ÇÄ = -Œ≥/s is trivial. Right before the simplification where we set Œ≥=0 we had:

  f_green(x‚ÇÄ,x‚ÇÅ) = œÉ(x‚ÇÅ w‚ÇÅ + w‚ÇÄ x‚ÇÄ + b)

The argument of the sigmoid has always been WX.

The likeliness for diagonal lines, with non-zero Œ≥, is therefore

  likeliness = Œ† ‚ßºt‚Çõ œÉ(W X‚Çõ) + (1 - t‚Çõ)(1 - œÉ(W X‚Çõ))‚ßΩ
               s

we find that

  ‚àÇz‚Çõ/‚àÇw‚ÇÄ = z‚Çõ (1 - z‚Çõ) x‚ÇÄ‚Çõ

and

  ‚àÇL/‚àÇw‚ÇÄ = ùö∫ ‚àÇL/‚àÇz‚Çõ ‚àÇz‚Çõ/‚àÇw‚ÇÄ = ùö∫ ((1 - t‚Çõ) z‚Çõ - t‚Çõ (1 - z‚Çõ)) x‚ÇÄ‚Çõ
           s                  s

as in README.back_propagation (which you should have read first).
We can include the bias into the weights matrix by setting w‚ÇÇ=b,
x‚ÇÇ=1 and then writing (see README.back_propagation):

  ‚àÇL/‚àÇw·µ¢‚±º = ‚àÇL/‚àÇz·µ¢ ‚àÇz·µ¢/‚àÇv·µ¢ ‚àÇv·µ¢/‚àÇw·µ¢‚±º = Œ¥‚ÇÅ·µ¢x‚±º

where

  Œ¥‚ÇÅ·µ¢ = ‚àÇL/‚àÇz·µ¢ ‚àÇz·µ¢/‚àÇv·µ¢

We only have one neuron (output) so i here is 0 (the index of the single neuron):

  ‚àÇL/‚àÇw‚ÇÄ‚±º = ‚àÇL/‚àÇz‚ÇÄ ‚àÇz‚ÇÄ/‚àÇw‚ÇÄ‚±º = Œ¥‚ÇÅ‚ÇÄx‚±º

(using that ‚àÇz·µ¢/‚àÇv·µ¢ ‚àÇv·µ¢/‚àÇw·µ¢‚±º = ‚àÇz·µ¢/‚àÇw·µ¢‚±º).
Using the backpropagation magic, we have the initial Œæ:

                            -1                -1                   -1
  Œæ‚ÇÄ‚Çõ = ‚àÇL/‚àÇz‚ÇÄ‚Çõ, which is ‚éº‚éº‚éº‚éº‚éº‚éº‚éº if t‚Çõ=0 and ‚éº‚éº‚éº if t‚Çõ=1, aka ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº
                          z‚ÇÄ‚Çõ - 1             z‚ÇÄ‚Çõ              z‚ÇÄ‚Çõ + t‚Çõ - 1

  Œ¥‚ÇÄ‚Çõ = Œæ‚ÇÄ‚Çõ ‚àÇœÉ/‚àÇz‚ÇÄ‚Çõ = Œæ‚ÇÄ‚Çõz‚ÇÄ‚Çõ(1 - z‚ÇÄ‚Çõ)

There is no need to calculate Œæ‚ÇÄ‚Çõ = W·µÄŒ¥ because we only have a single layer.

Finally, the gradient matrix ‚àÇL/‚àÇw‚ÇÄ‚±º is thus:

  G‚ÇÄ‚±º = Œ¥ X·µÄ

We can also combine the calculation of Œæ‚ÇÄ‚Çõ and Œ¥‚ÇÄ‚Çõ for the last layer,
and calculate the initial Œ¥‚ÇÄ‚Çõ as

  Œ¥‚ÇÄ‚Çõ = z‚ÇÄ‚Çõ if t‚Çõ=0 and (z‚ÇÄ‚Çõ-1) if t‚Çõ=1, aka Œ¥‚ÇÄ‚Çõ = z‚ÇÄ‚Çõ - t‚Çõ = residual‚ÇÄ‚Çõ.


The loss function
=================

Above we found

                   1 - 2t‚Çõ                     1               -1
  ‚àÇL/‚àÇz‚Çõ = ‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº‚éº, which is ‚éº‚éº‚éº‚éº‚éº‚éº if t=0 and ‚éº‚éº if t=1.
            1 - t‚Çõ - (1 - 2t‚Çõ) z‚Çõ            1 - z‚Çõ            z‚Çõ

Integrating this then gives

  L = - ùö∫ (t‚Çõ Log(z‚Çõ) + (1 - t‚Çõ) Log(1 - z‚Çõ))
        s

Low and behold, this is known as the "Binary Cross-Entropy Loss Function"
already widely used for binary classification tasks!
See for example https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy
